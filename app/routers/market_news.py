"""
å¸‚åœºå¿«è®¯APIè·¯ç”±
æä¾›è´¢è”ç¤¾ç”µæŠ¥ã€æ–°æµªè´¢ç»ã€ä¸œæ–¹è´¢å¯Œç½‘ç­‰å®æ—¶å¿«è®¯æ¥å£
"""
from fastapi import APIRouter, HTTPException, Depends, Query
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import logging
import httpx
import json
import re
import jieba
import jieba.analyse
from collections import Counter

from app.routers.auth_db import get_current_user
from app.core.response import ok
from app.core.database import get_mongo_db

router = APIRouter(prefix="/api/market-news", tags=["å¸‚åœºå¿«è®¯"])
logger = logging.getLogger("webapi")

# ç¼“å­˜å­˜å‚¨ï¼ˆå®é™…ç”Ÿäº§åº”ä½¿ç”¨Redisï¼‰
_telegraph_cache = {
    "è´¢è”ç¤¾ç”µæŠ¥": [],
    "æ–°æµªè´¢ç»": [],
    "ä¸œæ–¹è´¢å¯Œç½‘": []
}
_global_indexes_cache = None
_industry_rank_cache = []

# ç¼“å­˜æ—¶é—´
_cache_time = {
    "è´¢è”ç¤¾ç”µæŠ¥": None,
    "æ–°æµªè´¢ç»": None,
    "ä¸œæ–¹è´¢å¯Œç½‘": None
}


async def fetch_cailian_telegraph():
    """è·å–è´¢è”ç¤¾ç”µæŠ¥æ•°æ®"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                "https://www.cls.cn/nodeapi/telegraphList",
                headers={
                    "Referer": "https://www.cls.cn/",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
            )
            data = response.json()

            telegraphs = []
            if data.get("error") == 0 and data.get("data"):
                roll_data = data["data"].get("roll_data", [])
                for item in roll_data:
                    ctime = item.get("ctime", 0)
                    data_time = datetime.fromtimestamp(ctime)

                    # è·å–ä¸»é¢˜æ ‡ç­¾
                    subjects = []
                    if item.get("subjects"):
                        subjects = [s.get("subject_name", "") for s in item["subjects"]]

                    telegraphs.append({
                        "id": str(item.get("id", "")),
                        "title": item.get("title", ""),
                        "content": item.get("content", ""),
                        "time": data_time.strftime("%H:%M:%S"),
                        "dataTime": data_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "url": item.get("shareurl", ""),
                        "source": "è´¢è”ç¤¾ç”µæŠ¥",
                        "isRed": item.get("level") != "C",
                        "subjects": subjects
                    })
            return telegraphs
    except Exception as e:
        logger.error(f"è·å–è´¢è”ç¤¾ç”µæŠ¥å¤±è´¥: {e}")
        return []


async def fetch_sina_news():
    """è·å–æ–°æµªè´¢ç»æ•°æ®"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            timestamp = int(datetime.now().timestamp())
            url = f"https://zhibo.sina.com.cn/api/zhibo/feed?callback=callback&page=1&page_size=20&zhibo_id=152&tag_id=0&dire=f&dpc=1&type=0&_={timestamp}"

            response = await client.get(
                url,
                headers={
                    "Referer": "https://finance.sina.com.cn",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
            )

            # è§£æJSONPæ ¼å¼
            content = response.text
            content = content.replace("try{callback(", "").replace(");}catch(e){};", "")

            data = json.loads(content)
            telegraphs = []

            if data.get("result") and data["result"].get("data"):
                feed_data = data["result"]["data"].get("feed", {})
                feed_list = feed_data.get("list", [])
                for item in feed_list:
                    rich_text = item.get("rich_text", "")
                    # ä»ã€ã€‘ä¸­æå–æ ‡é¢˜
                    title_match = re.search(r"ã€(.*?)ã€‘", rich_text)
                    title = title_match.group(1) if title_match else ""

                    create_time = item.get("create_time", "")
                    time_str = create_time.split(" ")[1] if " " in create_time else ""

                    # è·å–æ ‡ç­¾
                    tags = item.get("tag", [])
                    subjects = [t.get("name", "") for t in tags if isinstance(t, dict)]

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç„¦ç‚¹æ–°é—»
                    is_focus = any(s.get("name") == "ç„¦ç‚¹" for s in tags if isinstance(s, dict))

                    telegraphs.append({
                        "id": str(item.get("id", "")),
                        "title": title,
                        "content": rich_text,
                        "time": time_str,
                        "dataTime": create_time,
                        "source": "æ–°æµªè´¢ç»",
                        "isRed": is_focus,
                        "subjects": subjects
                    })
            return telegraphs
    except Exception as e:
        logger.error(f"è·å–æ–°æµªè´¢ç»å¤±è´¥: {e}")
        return []


async def fetch_eastmoney_news(limit=50):
    """è·å–ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»æ•°æ®ï¼ˆä½¿ç”¨AKShareï¼‰"""
    try:
        import akshare as ak

        # è·å–æ–°é—»
        df_news = ak.stock_news_em()

        if df_news is None or df_news.empty:
            logger.warning("ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»æ•°æ®ä¸ºç©º")
            return []

        telegraphs = []
        for _, row in df_news.head(limit).iterrows():
            try:
                # è§£ææ—¶é—´
                publish_time = str(row.get('å‘å¸ƒæ—¶é—´') or row.get('time') or '')

                # å°è¯•è§£ææ—¶é—´æ ¼å¼
                time_str = publish_time
                data_time = publish_time
                try:
                    if ' ' in publish_time:
                        time_parts = publish_time.split(' ')
                        if len(time_parts) >= 2:
                            time_str = time_parts[1] if ':' in time_parts[1] else publish_time
                except:
                    pass

                title = str(row.get('æ–°é—»æ ‡é¢˜') or row.get('æ ‡é¢˜') or row.get('title') or '')
                content = title  # ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»åªæœ‰æ ‡é¢˜ï¼Œæ²¡æœ‰æ­£æ–‡
                source = str(row.get('æ–‡ç« æ¥æº') or row.get('æ¥æº') or row.get('source') or 'ä¸œæ–¹è´¢å¯Œç½‘')
                url = str(row.get('æ–°é—»é“¾æ¥') or row.get('url') or '')

                telegraphs.append({
                    "id": hash(url) if url else hash(title),
                    "title": title,
                    "content": content,
                    "time": time_str,
                    "dataTime": data_time,
                    "url": url,
                    "source": source,
                    "isRed": False,
                    "subjects": []
                })
            except Exception as e:
                logger.warning(f"è§£æä¸œæ–¹è´¢å¯Œç½‘æ–°é—»æ¡ç›®å¤±è´¥: {e}")
                continue

        logger.info(f"ä»ä¸œæ–¹è´¢å¯Œç½‘è·å–åˆ° {len(telegraphs)} æ¡æ–°é—»")
        return telegraphs

    except ImportError:
        logger.error("AKShareæœªå®‰è£…ï¼Œæ— æ³•è·å–ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»")
        return []
    except Exception as e:
        logger.error(f"è·å–ä¸œæ–¹è´¢å¯Œç½‘æ–°é—»å¤±è´¥: {e}")
        return []


async def save_news_to_db(news_list: List[Dict], source: str):
    """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
    try:
        db = get_mongo_db()
        collection = db.market_news

        # æ‰¹é‡æ’å…¥/æ›´æ–°æ–°é—»
        for news in news_list:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ ¹æ®URLæˆ–æ ‡é¢˜+æ—¶é—´ï¼‰
                existing = None
                if news.get("url"):
                    existing = await collection.find_one({"url": news["url"]})
                elif news.get("title") and news.get("dataTime"):
                    existing = await collection.find_one({
                        "title": news["title"],
                        "dataTime": news["dataTime"]
                    })

                news_doc = {
                    **news,
                    "source": source,
                    "createdAt": datetime.now(),
                    "updatedAt": datetime.now()
                }

                if existing:
                    # æ›´æ–°å·²å­˜åœ¨çš„æ–°é—»
                    await collection.update_one(
                        {"_id": existing["_id"]},
                        {"$set": news_doc}
                    )
                else:
                    # æ’å…¥æ–°æ–°é—»
                    await collection.insert_one(news_doc)

            except Exception as e:
                logger.warning(f"ä¿å­˜å•æ¡æ–°é—»å¤±è´¥: {e}")
                continue

        logger.info(f"æˆåŠŸä¿å­˜ {len(news_list)} æ¡ {source} æ–°é—»åˆ°æ•°æ®åº“")

    except Exception as e:
        logger.error(f"ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“å¤±è´¥: {e}")


async def fetch_global_indexes():
    """è·å–å…¨çƒè‚¡æŒ‡æ•°æ®"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                "https://proxy.finance.qq.com/ifzqgtimg/appstock/app/rank/indexRankDetail2",
                headers={
                    "Referer": "https://stockapp.finance.qq.com/mstats",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
            )
            data = response.json()

            if data.get("data"):
                return data["data"]
            return {}
    except Exception as e:
        logger.error(f"è·å–å…¨çƒè‚¡æŒ‡å¤±è´¥: {e}")
        return {}


async def fetch_industry_rank(sort: str = "0", count: int = 150):
    """è·å–è¡Œä¸šæ’åæ•°æ®"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            url = f"https://proxy.finance.qq.com/ifzqgtimg/appstock/app/mktHs/rank?l={count}&p=1&t=01/averatio&ordertype=&o={sort}"
            response = await client.get(
                url,
                headers={
                    "Referer": "https://stockapp.finance.qq.com/",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
            )
            data = response.json()

            if data.get("data"):
                return data["data"]
            return []
    except Exception as e:
        logger.error(f"è·å–è¡Œä¸šæ’åå¤±è´¥: {e}")
        return []


@router.get("/telegraph")
async def get_telegraph_list(
    source: str = Query(..., description="æ–°é—»æ¥æº"),
    current_user: dict = Depends(get_current_user)
):
    """è·å–ç”µæŠ¥åˆ—è¡¨"""
    try:
        # å¦‚æœç¼“å­˜ä¸ºç©ºæˆ–è¿‡æœŸï¼Œåˆ·æ–°æ•°æ®
        if not _telegraph_cache.get(source) or (
            _cache_time.get(source) and
            (datetime.now() - _cache_time[source]).seconds > 60
        ):
            await refresh_telegraph_data(source)

        data = _telegraph_cache.get(source, [])
        return ok(data=data, message=f"è·å–{source}æˆåŠŸ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/refresh")
async def refresh_telegraph_list(
    request: dict,
    current_user: dict = Depends(get_current_user)
):
    """åˆ·æ–°ç”µæŠ¥åˆ—è¡¨"""
    try:
        source = request.get("source", "è´¢è”ç¤¾ç”µæŠ¥")
        await refresh_telegraph_data(source)
        data = _telegraph_cache.get(source, [])
        return ok(data=data, message=f"åˆ·æ–°æˆåŠŸ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°å¤±è´¥: {str(e)}")


async def refresh_telegraph_data(source: str):
    """åˆ·æ–°æŒ‡å®šæ¥æºçš„æ•°æ®"""
    if source == "è´¢è”ç¤¾ç”µæŠ¥":
        news_list = await fetch_cailian_telegraph()
        _telegraph_cache[source] = news_list
        _cache_time[source] = datetime.now()
        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_news_to_db(news_list, "è´¢è”ç¤¾ç”µæŠ¥")
    elif source == "æ–°æµªè´¢ç»":
        news_list = await fetch_sina_news()
        _telegraph_cache[source] = news_list
        _cache_time[source] = datetime.now()
        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_news_to_db(news_list, "æ–°æµªè´¢ç»")
    elif source == "ä¸œæ–¹è´¢å¯Œç½‘":
        news_list = await fetch_eastmoney_news()
        _telegraph_cache[source] = news_list
        _cache_time[source] = datetime.now()
        # ä¿å­˜åˆ°æ•°æ®åº“
        await save_news_to_db(news_list, "ä¸œæ–¹è´¢å¯Œç½‘")


@router.get("/global-indexes")
async def get_global_stock_indexes(current_user: dict = Depends(get_current_user)):
    """è·å–å…¨çƒè‚¡æŒ‡æ•°æ®"""
    try:
        global _global_indexes_cache
        if _global_indexes_cache is None:
            _global_indexes_cache = await fetch_global_indexes()
        return ok(data=_global_indexes_cache, message="è·å–æˆåŠŸ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/industry-rank")
async def get_industry_rank(
    sort: str = Query("0"),
    count: int = Query(150),
    current_user: dict = Depends(get_current_user)
):
    """è·å–è¡Œä¸šæ’å"""
    try:
        global _industry_rank_cache
        if not _industry_rank_cache:
            _industry_rank_cache = await fetch_industry_rank(sort, count)

        return ok(data=_industry_rank_cache, message="è·å–æˆåŠŸ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/keywords")
async def get_news_keywords(
    hours: int = Query(24, description="ç»Ÿè®¡æœ€è¿‘å¤šå°‘å°æ—¶çš„å…³é”®è¯"),
    top_n: int = Query(50, description="è¿”å›å‰Nä¸ªå…³é”®è¯"),
    current_user: dict = Depends(get_current_user)
):
    """è·å–æ–°é—»å…³é”®è¯åˆ†æ"""
    try:
        db = get_mongo_db()
        collection = db.market_news

        # è®¡ç®—æ—¶é—´èŒƒå›´
        start_time = datetime.now() - timedelta(hours=hours)

        # ä»æ•°æ®åº“è·å–æœ€è¿‘çš„æ–°é—»
        cursor = await collection.find({
            "createdAt": {"$gte": start_time}
        }).to_list(None)

        # æå–æ‰€æœ‰æ ‡é¢˜å’Œå†…å®¹
        all_text = []
        for news in cursor:
            if news.get("title"):
                all_text.append(news["title"])
            if news.get("content"):
                all_text.append(news["content"])

        # ä½¿ç”¨jiebaè¿›è¡Œå…³é”®è¯åˆ†æ
        if not all_text:
            return ok(data={"keywords": [], "total_news": 0}, message="æš‚æ— æ–°é—»æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = " ".join(all_text)

        # æå–å…³é”®è¯
        keywords_with_scores = jieba.analyse.extract_tags(combined_text, topK=top_n, withWeight=True)

        keywords = [{"keyword": k, "weight": float(w)} for k, w in keywords_with_scores]

        # æŒ‰æ¥æºç»Ÿè®¡æ–°é—»æ•°é‡
        news_by_source = await collection.aggregate([
            {"$match": {"createdAt": {"$gte": start_time}}},
            {"$group": {"_id": "$source", "count": {"$sum": 1}}}
        ]).to_list(None)

        source_stats = {item["_id"]: item["count"] for item in news_by_source}

        return ok(data={
            "keywords": keywords,
            "total_news": len(cursor),
            "source_stats": source_stats,
            "time_range": f"æœ€è¿‘{hours}å°æ—¶"
        }, message="å…³é”®è¯åˆ†æå®Œæˆ")

    except Exception as e:
        logger.error(f"å…³é”®è¯åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@router.post("/ai-summary")
async def ai_summary_market_news(
    request: dict,
    current_user: dict = Depends(get_current_user)
):
    """AIå¸‚åœºèµ„è®¯æ€»ç»“"""
    try:
        question = request.get("question", "æ€»ç»“å’Œåˆ†æå½“å‰è‚¡ç¥¨å¸‚åœºæ–°é—»ä¸­çš„æŠ•èµ„æœºä¼šå’Œé£é™©ç‚¹")
        # TODO: è°ƒç”¨LLMæœåŠ¡ç”Ÿæˆæ€»ç»“
        summary = _mock_ai_summary(question)
        return ok(data={
            "content": summary["content"],
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "modelName": summary.get("model", "Qwen2.5-72B")
        }, message="AIæ€»ç»“ç”ŸæˆæˆåŠŸ")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


def _mock_ai_summary(question):
    return {
        "content": f"""# å¸‚åœºèµ„è®¯æ€»ç»“

## ä¸»è¦å¸‚åœºåŠ¨æ€

1. **Aè‚¡å¸‚åœºè¡¨ç°ç§¯æ**ï¼šä»Šæ—¥å¸‚åœºéœ‡è¡ä¸Šè¡Œï¼Œç§‘æŠ€æ¿å—è¡¨ç°å¼ºåŠ¿
2. **èµ„é‡‘æµå‘**ï¼šåŒ—å‘èµ„é‡‘å‡€æµå…¥è¶…50äº¿å…ƒï¼Œæ˜¾ç¤ºå¤–èµ„ä¿¡å¿ƒ
3. **æ”¿ç­–ç¯å¢ƒ**ï¼šå¤®è¡Œç»§ç»­å®æ–½ç¨³å¥è´§å¸æ”¿ç­–ï¼ŒæµåŠ¨æ€§å……è£•

## æŠ•èµ„æœºä¼š

- **ç§‘æŠ€æ¿å—**ï¼šåŠå¯¼ä½“ã€äººå·¥æ™ºèƒ½æ¦‚å¿µè¡¨ç°å¼ºåŠ¿ï¼Œå»ºè®®å…³æ³¨é¾™å¤´ä¸ªè‚¡
- **æ–°èƒ½æºè½¦**ï¼šé”€é‡æŒç»­æ”€å‡ï¼Œè¡Œä¸šæ™¯æ°”åº¦æå‡
- **åˆ¸å•†æ¿å—**ï¼šåˆåå¼‚åŠ¨æ‹‰å‡ï¼Œå¯èƒ½é¢„ç¤ºå¸‚åœºæƒ…ç»ªå¥½è½¬

## é£é™©æç¤º

- å¤–å›´å¸‚åœºæ³¢åŠ¨ï¼Œéœ€å…³æ³¨ç¾è”å‚¨æ”¿ç­–åŠ¨å‘
- éƒ¨åˆ†çƒ­é—¨æ¿å—çŸ­æœŸæ¶¨å¹…è¾ƒå¤§ï¼Œæ³¨æ„å›è°ƒé£é™©

*æ³¨ï¼šæœ¬åˆ†æåŸºäºå½“å‰å¸‚åœºå…¬å¼€ä¿¡æ¯ï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„éœ€è°¨æ…ï¼Œé£é™©è‡ªæ‹…ã€‚*
""",
        "model": "Qwen2.5-72B"
    }


@router.get("/grouped")
async def get_grouped_news(
    source: Optional[str] = Query(None, description="æ–°é—»æ¥æºï¼Œä¸æŒ‡å®šåˆ™è·å–æ‰€æœ‰æ¥æº"),
    strategy: str = Query("dynamic_hot", description="æ’åºç­–ç•¥: dynamic_hot(åŠ¨æ€çƒ­ç‚¹ä¼˜å…ˆ) æˆ– timeline(æ—¶é—´çº¿ä¼˜å…ˆ)"),
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–åˆ†ç»„èšåˆåçš„å¸‚åœºæ–°é—»

    æŒ‰ç…§ä»¥ä¸‹é€»è¾‘åˆ†ç»„:
    1. market_overview: å¸‚åœºå¤§ç›˜ä¸æŒ‡æ ‡ (å½±å“æ•´ä¸ªå¸‚åœºçš„æ¶ˆæ¯)
    2. hot_concepts: çƒ­ç‚¹æ¦‚å¿µ/é¢˜æé›†ç¾¤ (å½“æ—¥æœ€æ´»è·ƒçš„ä¸»é¢˜æŠ•èµ„çº¿)
    3. limit_up: æ¶¨åœä¸èµ„é‡‘åŠ¨å‘æ±‡æ€» (æ¶¨åœæ¿ã€å°å•ç­‰ç»Ÿè®¡)
    4. stock_alerts: ä¸ªè‚¡é‡è¦å…¬å‘Šä¸å¼‚åŠ¨ (å…·ä½“å…¬å¸çš„å…¬å‘Šã€é¾™è™æ¦œç­‰)
    5. fund_movements: èµ„é‡‘åŠ¨å‘æ±‡æ€» (è·¨è¶Šä¸åŒæ¿å—çš„ç»¼åˆæ€§èµ„é‡‘æŠ¥å‘Š)
    """
    try:
        from app.services.news_grouping_service import group_market_news

        # æ”¶é›†æ‰€æœ‰æ¥æºçš„æ–°é—»
        all_news = []

        # å¦‚æœæŒ‡å®šäº†æ¥æºï¼Œåªè·å–è¯¥æ¥æº
        if source:
            if not _telegraph_cache.get(source) or (
                _cache_time.get(source) and
                (datetime.now() - _cache_time[source]).seconds > 60
            ):
                await refresh_telegraph_data(source)
            all_news = _telegraph_cache.get(source, [])
        else:
            # è·å–æ‰€æœ‰æ¥æºçš„æ–°é—»
            for src in ["è´¢è”ç¤¾ç”µæŠ¥", "æ–°æµªè´¢ç»", "ä¸œæ–¹è´¢å¯Œç½‘"]:
                if not _telegraph_cache.get(src) or (
                    _cache_time.get(src) and
                    (datetime.now() - _cache_time[src]).seconds > 60
                ):
                    await refresh_telegraph_data(src)
                all_news.extend(_telegraph_cache.get(src, []))

        # å¦‚æœæ²¡æœ‰æ–°é—»ï¼Œè¿”å›ç©ºç»“æœ
        if not all_news:
            return ok(data={
                "market_overview": [],
                "hot_concepts": [],
                "stock_alerts": [],
                "fund_movements": [],
                "limit_up": [],
                "summary": {
                    "total_news": 0,
                    "market_overview_count": 0,
                    "hot_concept_count": 0,
                    "stock_alert_count": 0,
                    "fund_movement_count": 0,
                    "limit_up_count": 0,
                }
            }, message="æš‚æ— æ–°é—»æ•°æ®")

        # åº”ç”¨åˆ†ç»„èšåˆ
        grouped_result = group_market_news(all_news, strategy)

        return ok(data=grouped_result, message="è·å–åˆ†ç»„æ–°é—»æˆåŠŸ")

    except Exception as e:
        logger.error(f"è·å–åˆ†ç»„æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/refresh-grouped")
async def refresh_grouped_news(
    request: dict,
    current_user: dict = Depends(get_current_user)
):
    """
    åˆ·æ–°å¹¶è·å–åˆ†ç»„èšåˆåçš„å¸‚åœºæ–°é—»

    å¯ä»¥æŒ‡å®šåˆ·æ–°ç‰¹å®šçš„æ–°é—»æ¥æº
    """
    try:
        from app.services.news_grouping_service import group_market_news

        source = request.get("source")  # å¯é€‰ï¼šæŒ‡å®šåˆ·æ–°å“ªä¸ªæ¥æº
        strategy = request.get("strategy", "dynamic_hot")

        # åˆ·æ–°æŒ‡å®šæ¥æºæˆ–æ‰€æœ‰æ¥æº
        if source:
            await refresh_telegraph_data(source)
            all_news = _telegraph_cache.get(source, [])
        else:
            for src in ["è´¢è”ç¤¾ç”µæŠ¥", "æ–°æµªè´¢ç»", "ä¸œæ–¹è´¢å¯Œç½‘"]:
                await refresh_telegraph_data(src)
            all_news = []
            for src in ["è´¢è”ç¤¾ç”µæŠ¥", "æ–°æµªè´¢ç»", "ä¸œæ–¹è´¢å¯Œç½‘"]:
                all_news.extend(_telegraph_cache.get(src, []))

        # åº”ç”¨åˆ†ç»„èšåˆ
        grouped_result = group_market_news(all_news, strategy)

        return ok(data=grouped_result, message="åˆ·æ–°å¹¶è·å–åˆ†ç»„æ–°é—»æˆåŠŸ")

    except Exception as e:
        logger.error(f"åˆ·æ–°åˆ†ç»„æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°å¤±è´¥: {str(e)}")


# ==================== å¢å¼ºæ•°æ®åº“ API ====================

@router.on_event("startup")
async def init_news_database():
    """åˆå§‹åŒ–æ–°é—»æ•°æ®åº“ç´¢å¼•"""
    try:
        from app.services.news_database_service import NewsDatabaseService
        await NewsDatabaseService.ensure_indexes()
        logger.info("æ–°é—»æ•°æ®åº“ç´¢å¼•åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.warning(f"æ–°é—»æ•°æ®åº“ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")


@router.get("/analytics")
async def get_news_analytics(
    hours: int = Query(24, description="ç»Ÿè®¡æœ€è¿‘å¤šå°‘å°æ—¶çš„æ•°æ®"),
    source: Optional[str] = Query(None, description="æŒ‡å®šæ¥æº"),
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–æ–°é—»åˆ†ææ•°æ®
    
    è¿”å›:
    - æ€»æ•°ç»Ÿè®¡
    - æ¥æºåˆ†å¸ƒ
    - åˆ†ç±»åˆ†å¸ƒ
    - æƒ…æ„Ÿåˆ†å¸ƒ
    - çƒ­é—¨è‚¡ç¥¨
    - çƒ­é—¨æ¦‚å¿µ
    - è¯äº‘æ•°æ®
    """
    try:
        from app.services.news_database_service import NewsDatabaseService
        from datetime import timedelta
        
        start_date = datetime.now() - timedelta(hours=hours)
        
        sources = [source] if source else None
        analytics = await NewsDatabaseService.get_news_analytics(
            start_date=start_date,
            sources=sources
        )
        
        return ok(data=analytics, message="è·å–åˆ†ææ•°æ®æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–æ–°é—»åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/enhanced-wordcloud")
async def get_enhanced_wordcloud(
    hours: int = Query(24, description="ç»Ÿè®¡æœ€è¿‘å¤šå°‘å°æ—¶"),
    top_n: int = Query(50, description="è¿”å›å‰Nä¸ªè¯"),
    source: Optional[str] = Query(None, description="æŒ‡å®šæ¥æº"),
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–å¢å¼ºè¯äº‘æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨é¢„èšåˆç¼“å­˜ï¼‰

    åŸºäºæ•°æ®åº“ä¸­å­˜å‚¨çš„æ–°é—»å…³é”®è¯ç”Ÿæˆè¯äº‘ï¼Œæ”¯æŒ:
    - æƒé‡è®¡ç®—
    - åˆ†ç±»è¿‡æ»¤
    - æ—¶é—´èŒƒå›´

    æ€§èƒ½ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œç¼“å­˜å‘½ä¸­æ—¶å“åº”æ—¶é—´ < 100ms
    """
    try:
        # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨è¯äº‘ç¼“å­˜æœåŠ¡
        from app.services.wordcloud_cache_service import WordcloudCacheService

        wordcloud_data = await WordcloudCacheService.get_wordcloud_data(
            hours=hours,
            top_n=top_n,
            source=source
        )

        return ok(data={
            "words": wordcloud_data,
            "total": len(wordcloud_data),
            "hours": hours,
            "source": source or "å…¨éƒ¨",
            "cached": "æ˜¯" if len(wordcloud_data) > 0 else "å¦"
        }, message="è·å–è¯äº‘æ•°æ®æˆåŠŸ")

    except Exception as e:
        logger.error(f"è·å–è¯äº‘æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/search")
async def search_news(
    keyword: str = Query(..., description="æœç´¢å…³é”®è¯"),
    limit: int = Query(50, description="è¿”å›æ•°é‡é™åˆ¶"),
    current_user: dict = Depends(get_current_user)
):
    """
    æœç´¢æ–°é—»
    
    æ”¯æŒåœ¨æ ‡é¢˜ã€å†…å®¹ã€å…³é”®è¯ã€æ ‡ç­¾ä¸­æœç´¢
    """
    try:
        from app.services.news_database_service import NewsDatabaseService
        
        results = await NewsDatabaseService.search_news(
            keyword=keyword,
            limit=limit
        )
        
        return ok(data={
            "keyword": keyword,
            "count": len(results),
            "results": results
        }, message=f"æœç´¢åˆ° {len(results)} æ¡ç»“æœ")
        
    except Exception as e:
        logger.error(f"æœç´¢æ–°é—»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/sync-to-enhanced-db")
async def sync_to_enhanced_database(
    hours: int = Query(24, description="åŒæ­¥æœ€è¿‘å¤šå°‘å°æ—¶çš„æ•°æ®"),
    current_user: dict = Depends(get_current_user)
):
    """
    å°†ç°æœ‰æ•°æ®åŒæ­¥åˆ°å¢å¼ºæ•°æ®åº“
    
    ä»market_newsé›†åˆè¯»å–æ•°æ®ï¼Œé‡æ–°æå–æ ‡ç­¾åä¿å­˜åˆ°market_news_enhanced
    """
    try:
        from app.services.news_database_service import NewsDatabaseService
        
        db = get_mongo_db()
        old_collection = db.market_news
        
        # æŸ¥è¯¢æœ€è¿‘çš„æ•°æ®
        cutoff_time = datetime.now() - timedelta(hours=hours)
        cursor = old_collection.find({"createdAt": {"$gte": cutoff_time}}).limit(500)
        
        synced_count = 0
        async for doc in cursor:
            source = doc.get("source", "æœªçŸ¥")
            news_dict = {
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "url": doc.get("url"),
                "time": doc.get("time", ""),
                "dataTime": doc.get("dataTime"),
                "isRed": doc.get("isRed", False),
                "subjects": doc.get("subjects", [])
            }
            
            count = await NewsDatabaseService.save_news([news_dict], source)
            synced_count += count
        
        return ok(data={
            "synced_count": synced_count,
            "hours": hours
        }, message=f"æˆåŠŸåŒæ­¥ {synced_count} æ¡æ•°æ®")
        
    except Exception as e:
        logger.error(f"åŒæ­¥æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")
