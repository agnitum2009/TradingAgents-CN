"""
å¢žå¼ºç‰ˆé˜Ÿåˆ—æœåŠ¡
åŸºäºŽçŽ°æœ‰å®žçŽ°ï¼Œæ·»åŠ å¹¶å‘æŽ§åˆ¶ã€ä¼˜å…ˆçº§é˜Ÿåˆ—ã€å¯è§æ€§è¶…æ—¶ç­‰åŠŸèƒ½
"""

import json
import time
import uuid
import asyncio
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta

from redis.asyncio import Redis

from app.core.database import get_redis_client

from app.services.queue import (
    READY_LIST,
    TASK_PREFIX,
    BATCH_PREFIX,
    SET_PROCESSING,
    SET_COMPLETED,
    SET_FAILED,
    BATCH_TASKS_PREFIX,
    USER_PROCESSING_PREFIX,
    GLOBAL_CONCURRENT_KEY,
    VISIBILITY_TIMEOUT_PREFIX,
    DEFAULT_USER_CONCURRENT_LIMIT,
    GLOBAL_CONCURRENT_LIMIT,
    VISIBILITY_TIMEOUT_SECONDS,
    check_user_concurrent_limit,
    check_global_concurrent_limit,
    mark_task_processing,
    unmark_task_processing,
    set_visibility_timeout,
    clear_visibility_timeout,
)

logger = logging.getLogger(__name__)

# Redisé”®åä¸Žé…ç½®å¸¸é‡ç”± app.services.queue.keys æä¾›ï¼ˆæ­¤å¤„ä¸å†é‡å¤å®šä¹‰ï¼‰


class QueueService:
    """å¢žå¼ºç‰ˆé˜Ÿåˆ—æœåŠ¡ç±»"""

    def __init__(self, redis: Redis):
        self.r = redis
        self.user_concurrent_limit = DEFAULT_USER_CONCURRENT_LIMIT
        self.global_concurrent_limit = GLOBAL_CONCURRENT_LIMIT
        self.visibility_timeout = VISIBILITY_TIMEOUT_SECONDS

    async def enqueue_task(
        self,
        user_id: str,
        symbol: str,
        params: Dict[str, Any],
        batch_id: Optional[str] = None
    ) -> str:
        """ä»»åŠ¡å…¥é˜Ÿï¼Œæ”¯æŒå¹¶å‘æŽ§åˆ¶ï¼ˆå¼€æºç‰ˆFIFOé˜Ÿåˆ—ï¼‰"""

        # æ£€æŸ¥ç”¨æˆ·å¹¶å‘é™åˆ¶
        if not await self._check_user_concurrent_limit(user_id):
            raise ValueError(f"ç”¨æˆ· {user_id} è¾¾åˆ°å¹¶å‘é™åˆ¶ ({self.user_concurrent_limit})")

        # æ£€æŸ¥å…¨å±€å¹¶å‘é™åˆ¶
        if not await self._check_global_concurrent_limit():
            raise ValueError(f"ç³»ç»Ÿè¾¾åˆ°å…¨å±€å¹¶å‘é™åˆ¶ ({self.global_concurrent_limit})")

        task_id = str(uuid.uuid4())
        key = TASK_PREFIX + task_id
        now = int(time.time())

        mapping = {
            "id": task_id,
            "user": user_id,
            "symbol": symbol,
            "status": "queued",
            "created_at": str(now),
            "params": json.dumps(params or {}),
            "enqueued_at": str(now)
        }

        if batch_id:
            mapping["batch_id"] = batch_id

        # ä¿å­˜ä»»åŠ¡æ•°æ®
        await self.r.hset(key, mapping=mapping)

        # æ·»åŠ åˆ°FIFOé˜Ÿåˆ—
        await self.r.lpush(READY_LIST, task_id)

        if batch_id:
            await self.r.sadd(BATCH_TASKS_PREFIX + batch_id, task_id)

        logger.info(f"ä»»åŠ¡å·²å…¥é˜Ÿ: {task_id}")
        return task_id

    async def dequeue_task(self, worker_id: str) -> Optional[Dict[str, Any]]:
        """ä»ŽFIFOé˜Ÿåˆ—ä¸­å–å‡ºä»»åŠ¡"""
        try:
            # ä»ŽFIFOé˜Ÿåˆ—èŽ·å–ä»»åŠ¡
            task_id = await self.r.rpop(READY_LIST)
            if not task_id:
                return None

            # èŽ·å–ä»»åŠ¡è¯¦æƒ…
            task_data = await self.get_task(task_id)
            if not task_data:
                logger.warning(f"ä»»åŠ¡æ•°æ®ä¸å­˜åœ¨: {task_id}")
                return None

            user_id = task_data.get("user")

            # å†æ¬¡æ£€æŸ¥å¹¶å‘é™åˆ¶ï¼ˆé˜²æ­¢ç«žæ€æ¡ä»¶ï¼‰
            if not await self._check_user_concurrent_limit(user_id):
                # å¦‚æžœè¶…è¿‡é™åˆ¶ï¼Œå°†ä»»åŠ¡æ”¾å›žé˜Ÿåˆ—
                await self.r.lpush(READY_LIST, task_id)
                logger.warning(f"ç”¨æˆ· {user_id} å¹¶å‘é™åˆ¶ï¼Œä»»åŠ¡é‡æ–°å…¥é˜Ÿ: {task_id}")
                return None

            # æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­
            await self._mark_task_processing(task_id, user_id, worker_id)

            # è®¾ç½®å¯è§æ€§è¶…æ—¶
            await self._set_visibility_timeout(task_id, worker_id)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self.r.hset(TASK_PREFIX + task_id, mapping={
                "status": "processing",
                "worker_id": worker_id,
                "started_at": str(int(time.time()))
            })

            logger.info(f"ä»»åŠ¡å·²å‡ºé˜Ÿ: {task_id} -> Worker: {worker_id}")
            return task_data

        except Exception as e:
            logger.error(f"å‡ºé˜Ÿå¤±è´¥: {e}")
            return None

    async def ack_task(self, task_id: str, success: bool = True) -> bool:
        """ç¡®è®¤ä»»åŠ¡å®Œæˆ"""
        try:
            task_data = await self.get_task(task_id)
            if not task_data:
                return False

            user_id = task_data.get("user")
            worker_id = task_data.get("worker_id")

            # ä»Žå¤„ç†ä¸­é›†åˆç§»é™¤
            await self._unmark_task_processing(task_id, user_id)

            # æ¸…é™¤å¯è§æ€§è¶…æ—¶
            await self._clear_visibility_timeout(task_id)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            status = "completed" if success else "failed"
            await self.r.hset(TASK_PREFIX + task_id, mapping={
                "status": status,
                "completed_at": str(int(time.time()))
            })

            # æ·»åŠ åˆ°ç›¸åº”çš„é›†åˆ
            if success:
                await self.r.sadd(SET_COMPLETED, task_id)
            else:
                await self.r.sadd(SET_FAILED, task_id)

            logger.info(f"ä»»åŠ¡å·²ç¡®è®¤: {task_id} (æˆåŠŸ: {success})")
            return True

        except Exception as e:
            logger.error(f"ç¡®è®¤ä»»åŠ¡å¤±è´¥: {e}")
            return False

    async def create_batch(self, user_id: str, symbols: List[str], params: Dict[str, Any]) -> tuple[str, int]:
        """
        åˆ›å»ºæ‰¹é‡åˆ†æžä»»åŠ¡ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
        ä½¿ç”¨ Redis Pipeline æ‰¹é‡æ“ä½œï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
        """
        batch_id = str(uuid.uuid4())
        now = int(time.time())

        # ðŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ Redis Pipeline æ‰¹é‡æ“ä½œï¼ˆå°†å¤šä¸ªå‘½ä»¤åˆå¹¶ä¸ºä¸€æ¬¡ç½‘ç»œå¾€è¿”ï¼‰
        pipe = self.r.pipeline(transaction=True)

        task_ids = []
        params_json = json.dumps(params or {})

        for symbol in symbols:
            task_id = str(uuid.uuid4())
            task_ids.append(task_id)

            key = TASK_PREFIX + task_id

            # æ‰¹é‡æ·»åŠ åˆ° pipelineï¼ˆä¸ç«‹å³æ‰§è¡Œï¼‰
            pipe.hset(key, mapping={
                "id": task_id,
                "user": user_id,
                "symbol": symbol,
                "status": "queued",
                "created_at": str(now),
                "params": params_json,
                "enqueued_at": str(now),
                "batch_id": batch_id
            })
            pipe.lpush(READY_LIST, task_id)
            pipe.sadd(BATCH_TASKS_PREFIX + batch_id, task_id)

        # ä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰å‘½ä»¤ï¼ˆåŽŸå­æ€§äº‹åŠ¡ï¼‰
        await pipe.execute()

        # æ‰¹é‡ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯
        batch_key = BATCH_PREFIX + batch_id
        await self.r.hset(batch_key, mapping={
            "id": batch_id,
            "user": user_id,
            "status": "queued",
            "submitted": str(len(symbols)),
            "created_at": str(now),
        })

        logger.info(f"âœ… æ‰¹é‡ä»»åŠ¡å·²å…¥é˜Ÿ: {batch_id} - {len(symbols)}ä¸ªè‚¡ç¥¨ (Pipelineä¼˜åŒ–)")
        return batch_id, len(symbols)

    async def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        key = TASK_PREFIX + task_id
        data = await self.r.hgetall(key)
        if not data:
            return None
        # parse fields
        if "params" in data:
            try:
                data["parameters"] = json.loads(data.pop("params"))
            except Exception:
                data["parameters"] = {}
        if "created_at" in data and data["created_at"].isdigit():
            data["created_at"] = int(data["created_at"])
        if "submitted" in data and str(data["submitted"]).isdigit():
            data["submitted"] = int(data["submitted"])
        return data

    async def get_batch(self, batch_id: str) -> Optional[Dict[str, Any]]:
        key = BATCH_PREFIX + batch_id
        data = await self.r.hgetall(key)
        if not data:
            return None
        # enrich with tasks count if set exists
        submitted = data.get("submitted")
        if submitted is not None and str(submitted).isdigit():
            data["submitted"] = int(submitted)
        if "created_at" in data and data["created_at"].isdigit():
            data["created_at"] = int(data["created_at"])
        data["tasks"] = list(await self.r.smembers(BATCH_TASKS_PREFIX + batch_id))
        return data

    async def stats(self) -> Dict[str, int]:
        queued = await self.r.llen(READY_LIST)
        processing = await self.r.scard(SET_PROCESSING)
        completed = await self.r.scard(SET_COMPLETED)
        failed = await self.r.scard(SET_FAILED)
        return {
            "queued": int(queued or 0),
            "processing": int(processing or 0),
            "completed": int(completed or 0),
            "failed": int(failed or 0),
        }

    # æ–°å¢žï¼šå¹¶å‘æŽ§åˆ¶æ–¹æ³•
    async def _check_user_concurrent_limit(self, user_id: str) -> bool:
        """æ£€æŸ¥ç”¨æˆ·å¹¶å‘é™åˆ¶ï¼ˆå§”æ‰˜ helpersï¼‰"""
        return await check_user_concurrent_limit(self.r, user_id, self.user_concurrent_limit)

    async def _check_global_concurrent_limit(self) -> bool:
        """æ£€æŸ¥å…¨å±€å¹¶å‘é™åˆ¶ï¼ˆå§”æ‰˜ helpersï¼‰"""
        return await check_global_concurrent_limit(self.r, self.global_concurrent_limit)

    async def _mark_task_processing(self, task_id: str, user_id: str, worker_id: str):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­ï¼ˆå§”æ‰˜ helpersï¼‰"""
        await mark_task_processing(self.r, task_id, user_id)

    async def _unmark_task_processing(self, task_id: str, user_id: str):
        """å–æ¶ˆä»»åŠ¡å¤„ç†ä¸­æ ‡è®°ï¼ˆå§”æ‰˜ helpersï¼‰"""
        await unmark_task_processing(self.r, task_id, user_id)

    async def _set_visibility_timeout(self, task_id: str, worker_id: str):
        """è®¾ç½®å¯è§æ€§è¶…æ—¶ï¼ˆå§”æ‰˜ helpersï¼‰"""
        await set_visibility_timeout(self.r, task_id, worker_id, self.visibility_timeout)

    async def _clear_visibility_timeout(self, task_id: str):
        """æ¸…é™¤å¯è§æ€§è¶…æ—¶"""
        await clear_visibility_timeout(self.r, task_id)

    async def get_user_queue_status(self, user_id: str) -> Dict[str, int]:
        """èŽ·å–ç”¨æˆ·é˜Ÿåˆ—çŠ¶æ€"""
        user_processing_key = USER_PROCESSING_PREFIX + user_id
        processing_count = await self.r.scard(user_processing_key)

        return {
            "processing": int(processing_count or 0),
            "concurrent_limit": self.user_concurrent_limit,
            "available_slots": max(0, self.user_concurrent_limit - int(processing_count or 0))
        }

    async def cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡ï¼ˆå¯è§æ€§è¶…æ—¶ï¼‰"""
        try:
            # èŽ·å–æ‰€æœ‰å¯è§æ€§è¶…æ—¶é”®
            timeout_keys = await self.r.keys(VISIBILITY_TIMEOUT_PREFIX + "*")

            current_time = int(time.time())
            expired_tasks = []

            for timeout_key in timeout_keys:
                timeout_data = await self.r.hgetall(timeout_key)
                if timeout_data:
                    timeout_at = int(timeout_data.get("timeout_at", 0))
                    if current_time > timeout_at:
                        task_id = timeout_data.get("task_id")
                        if task_id:
                            expired_tasks.append(task_id)

            # å¤„ç†è¿‡æœŸä»»åŠ¡
            for task_id in expired_tasks:
                await self._handle_expired_task(task_id)

            if expired_tasks:
                logger.warning(f"å¤„ç†äº† {len(expired_tasks)} ä¸ªè¿‡æœŸä»»åŠ¡")

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸä»»åŠ¡å¤±è´¥: {e}")

    async def _handle_expired_task(self, task_id: str):
        """å¤„ç†è¿‡æœŸä»»åŠ¡"""
        try:
            task_data = await self.get_task(task_id)
            if not task_data:
                return

            user_id = task_data.get("user")

            # ä»Žå¤„ç†ä¸­é›†åˆç§»é™¤
            await self._unmark_task_processing(task_id, user_id)

            # æ¸…é™¤å¯è§æ€§è¶…æ—¶
            await self._clear_visibility_timeout(task_id)

            # é‡æ–°åŠ å…¥é˜Ÿåˆ—
            await self.r.lpush(READY_LIST, task_id)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self.r.hset(TASK_PREFIX + task_id, mapping={
                "status": "queued",
                "worker_id": "",
                "requeued_at": str(int(time.time()))
            })

            logger.warning(f"è¿‡æœŸä»»åŠ¡é‡æ–°å…¥é˜Ÿ: {task_id}")

        except Exception as e:
            logger.error(f"å¤„ç†è¿‡æœŸä»»åŠ¡å¤±è´¥: {task_id} - {e}")

    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        try:
            task_data = await self.get_task(task_id)
            if not task_data:
                return False

            status = task_data.get("status")
            user_id = task_data.get("user")

            if status == "processing":
                # å¦‚æžœæ­£åœ¨å¤„ç†ä¸­ï¼Œä»Žå¤„ç†é›†åˆç§»é™¤
                await self._unmark_task_processing(task_id, user_id)
                await self._clear_visibility_timeout(task_id)
            elif status == "queued":
                # å¦‚æžœåœ¨é˜Ÿåˆ—ä¸­ï¼Œä»Žé˜Ÿåˆ—ç§»é™¤
                await self.r.lrem(READY_LIST, 0, task_id)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self.r.hset(TASK_PREFIX + task_id, mapping={
                "status": "cancelled",
                "cancelled_at": str(int(time.time()))
            })

            logger.info(f"ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
            return True

        except Exception as e:
            logger.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {e}")
            return False


def get_queue_service() -> QueueService:
    return QueueService(get_redis_client())