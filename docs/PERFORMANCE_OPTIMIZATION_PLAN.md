# æ€§èƒ½ä¼˜åŒ–å®æ–½æ–¹æ¡ˆ

> **é¡¹ç›®**: TradingAgents-CN
> **ç‰ˆæœ¬**: v1.0.3
> **æ—¥æœŸ**: 2026-01-17

---

## ä¼˜åŒ–æ¦‚è§ˆ

æœ¬æ–¹æ¡ˆå°†å®æ–½ 12 é¡¹æ€§èƒ½ä¼˜åŒ–ï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ä¸º 3 ä¸ªé˜¶æ®µï¼š

| é˜¶æ®µ | ä¼˜åŒ–é¡¹ | é¢„è®¡æ”¶ç›Š | é¢„è®¡å·¥æ—¶ |
|------|--------|----------|----------|
| **P0 (ç´§æ€¥)** | 4é¡¹ | å“åº”æ—¶é—´-75%, ååé‡+10å€ | 8å°æ—¶ |
| **P1 (é‡è¦)** | 6é¡¹ | é…ç½®è¯»å–-98%, æ•°æ®åº“è´Ÿè½½-50% | 10å°æ—¶ |
| **P2 (å»ºè®®)** | 2é¡¹ | æ—¥å¿—è¾“å‡º-70%, åˆ†é¡µæ€§èƒ½+50% | 4å°æ—¶ |

**æ€»é¢„è®¡å·¥æ—¶**: 3-4 ä¸ªå·¥ä½œæ—¥

---

## P0 ä¼˜åŒ– - ç«‹å³æ‰§è¡Œ

### ä¼˜åŒ– 1: ä¿®å¤ N+1 æŸ¥è¯¢é—®é¢˜

**æ–‡ä»¶**: `app/services/stock_data_service.py`

**ä¿®æ”¹ä½ç½®**: ç¬¬ 58-78 è¡Œ `get_stock_basic_info` æ–¹æ³•

**å½“å‰ä»£ç **:
```python
# æœªæŒ‡å®šæ•°æ®æºï¼ŒæŒ‰ä¼˜å…ˆçº§æŸ¥è¯¢
source_priority = ["tushare", "multi_source", "akshare", "baostock"]
doc = None

for src in source_priority:
    query_with_source = query.copy()
    query_with_source["source"] = src
    doc = await db[self.basic_info_collection].find_one(query_with_source, {"_id": 0})
    if doc:
        logger.debug(f"âœ… ä½¿ç”¨æ•°æ®æº: {src}")
        break
```

**ä¼˜åŒ–åä»£ç **:
```python
# ä½¿ç”¨èšåˆç®¡é“ä¸€æ¬¡æ€§æŒ‰ä¼˜å…ˆçº§æŸ¥è¯¢
source_priority = {"tushare": 1, "multi_source": 2, "akshare": 3, "baostock": 4}

pipeline = [
    {
        "$match": {
            "$or": [{"symbol": symbol6}, {"code": symbol6}]
        }
    },
    {
        "$addFields": {
            "sourcePriority": {
                "$switch": {
                    "branches": [
                        {"case": {"$eq": ["$source", "tushare"]}, "then": 1},
                        {"case": {"$eq": ["$source", "multi_source"]}, "then": 2},
                        {"case": {"$eq": ["$source", "akshare"]}, "then": 3},
                        {"case": {"$eq": ["$source", "baostock"]}, "then": 4}
                    ],
                    "default": 999
                }
            }
        }
    },
    {"$sort": {"sourcePriority": 1}},
    {"$limit": 1},
    {"$project": {"_id": 0}}
]

cursor = db[self.basic_info_collection].aggregate(pipeline)
results = await cursor.to_list(length=1)
doc = results[0] if results else None

if doc:
    logger.debug(f"âœ… ä½¿ç”¨æ•°æ®æº: {doc.get('source')}")
```

**æµ‹è¯•æ–¹æ¡ˆ**:
```python
# æµ‹è¯•è„šæœ¬
async def test_stock_query_performance():
    import time

    # æµ‹è¯• 100 æ¬¡æŸ¥è¯¢
    symbols = [f"{str(i).zfill(6)}" for i in range(1, 101)]

    start = time.time()
    for symbol in symbols:
        await service.get_stock_basic_info(symbol)
    elapsed = time.time() - start

    print(f"100æ¬¡æŸ¥è¯¢è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"å¹³å‡æ¯æ¬¡: {elapsed/100*1000:.2f}æ¯«ç§’")
```

---

### ä¼˜åŒ– 2: ä¼˜åŒ–æ‰¹é‡ä»»åŠ¡å…¥é˜Ÿ

**æ–‡ä»¶**: `app/services/queue_service.py`

**ä¿®æ”¹ä½ç½®**: ç¬¬ 179-192 è¡Œ `create_batch` æ–¹æ³•

**å½“å‰ä»£ç **:
```python
async def create_batch(self, user_id: str, symbols: List[str], params: Dict[str, Any]) -> tuple[str, int]:
    batch_id = str(uuid.uuid4())
    # ... åˆå§‹åŒ–ä»£ç  ...
    for s in symbols:
        await self.enqueue_task(user_id=user_id, symbol=s, params=params, batch_id=batch_id)
    return batch_id, len(symbols)
```

**ä¼˜åŒ–åä»£ç **:
```python
async def create_batch(self, user_id: str, symbols: List[str], params: Dict[str, Any]) -> tuple[str, int]:
    batch_id = str(uuid.uuid4())
    now = int(time.time())

    # ä½¿ç”¨ Redis Pipeline æ‰¹é‡æ“ä½œ
    pipe = self.r.pipeline(transaction=True)

    task_ids = []
    for symbol in symbols:
        task_id = str(uuid.uuid4())
        task_ids.append(task_id)

        key = TASK_PREFIX + task_id
        params_json = json.dumps(params or {})

        # æ‰¹é‡æ·»åŠ åˆ° pipelineï¼ˆä¸ç«‹å³æ‰§è¡Œï¼‰
        pipe.hset(key, mapping={
            "id": task_id,
            "user": user_id,
            "symbol": symbol,
            "status": "queued",
            "created_at": str(now),
            "params": params_json,
            "enqueued_at": str(now),
            "batch_id": batch_id
        })
        pipe.lpush(READY_LIST, task_id)
        pipe.sadd(BATCH_TASKS_PREFIX + batch_id, task_id)

    # ä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰å‘½ä»¤
    await pipe.execute()

    # æ‰¹é‡ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯
    batch_key = BATCH_PREFIX + batch_id
    await self.r.hset(batch_key, mapping={
        "id": batch_id,
        "user": user_id,
        "status": "queued",
        "submitted": str(len(symbols)),
        "created_at": str(now),
    })

    logger.info(f"æ‰¹é‡ä»»åŠ¡å·²å…¥é˜Ÿ: {batch_id} - {len(symbols)}ä¸ªè‚¡ç¥¨")
    return batch_id, len(symbols)
```

---

### ä¼˜åŒ– 3: é¢„èšåˆè¯äº‘æ•°æ®

**æ–°å»ºæ–‡ä»¶**: `app/services/wordcloud_cache_service.py`

```python
"""
è¯äº‘ç¼“å­˜æœåŠ¡
å®šæ—¶é¢„è®¡ç®—è¯äº‘æ•°æ®ï¼Œå‡å°‘å®æ—¶æŸ¥è¯¢å‹åŠ›
"""
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.database import get_mongo_db

logger = logging.getLogger(__name__)


class WordcloudCacheService:
    """è¯äº‘ç¼“å­˜æœåŠ¡"""

    CACHE_COLLECTION = "wordcloud_cache"
    CACHE_TTL_HOURS = 1  # ç¼“å­˜1å°æ—¶

    @classmethod
    async def ensure_indexes(cls):
        """åˆ›å»ºç´¢å¼•"""
        db = get_mongo_db()
        collection = db[cls.CACHE_COLLECTION]
        await collection.create_index([("type", 1), ("period", 1)])
        await collection.create_index("updated_at")

    @classmethod
    async def get_cached_wordcloud(
        cls,
        hours: int = 24,
        source: Optional[str] = None
    ) -> Optional[List[Dict]]:
        """è·å–ç¼“å­˜çš„è¯äº‘æ•°æ®"""
        try:
            db = get_mongo_db()
            collection = db[cls.CACHE_COLLECTION]

            # ç”Ÿæˆç¼“å­˜ key
            cache_key = f"wordcloud_{hours}h"
            if source:
                cache_key += f"_{source}"

            # æŸ¥è¯¢ç¼“å­˜
            cached = await collection.find_one({
                "type": cache_key,
                "updated_at": {"$gte": datetime.now() - timedelta(hours=cls.CACHE_TTL_HOURS)}
            })

            if cached:
                logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜è¯äº‘æ•°æ®: {cache_key}")
                return cached.get("data", [])

            return None

        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜è¯äº‘å¤±è´¥: {e}")
            return None

    @classmethod
    async def precompute_wordcloud(cls):
        """é¢„è®¡ç®—è¯äº‘æ•°æ®ï¼ˆå®šæ—¶ä»»åŠ¡è°ƒç”¨ï¼‰"""
        try:
            db = get_mongo_db()
            news_collection = db["market_news_enhanced"]
            cache_collection = db[cls.CACHE_COLLECTION]

            # é¢„è®¡ç®—å¤šä¸ªæ—¶é—´èŒƒå›´
            periods = [24, 48, 168]  # 1å¤©ã€2å¤©ã€1å‘¨

            for hours in periods:
                # æŒ‰æ¥æºåˆ†ç»„é¢„è®¡ç®—
                for source in [None, "eastmoney", "10jqka", "cls"]:
                    query = {
                        "dataTime": {"$gte": datetime.now() - timedelta(hours=hours)}
                    }
                    if source:
                        query["source"] = source

                    pipeline = [
                        {"$match": query},
                        {"$unwind": "$keywords"},
                        {"$group": {"_id": "$keywords", "count": {"$sum": 1}}},
                        {"$sort": {"count": -1}},
                        {"$limit": 200}  # ç¼“å­˜æ›´å¤š
                    ]

                    results = []
                    async for doc in news_collection.aggregate(pipeline):
                        results.append({
                            "word": doc["_id"],
                            "weight": doc["count"],
                            "count": doc["count"]
                        })

                    # ç”Ÿæˆç¼“å­˜ key
                    cache_key = f"wordcloud_{hours}h"
                    if source:
                        cache_key += f"_{source}"

                    # æ›´æ–°ç¼“å­˜
                    await cache_collection.update_one(
                        {"type": cache_key},
                        {
                            "$set": {
                                "type": cache_key,
                                "period": hours,
                                "source": source,
                                "data": results,
                                "updated_at": datetime.now()
                            }
                        },
                        upsert=True
                    )

                    logger.info(f"âœ… é¢„è®¡ç®—è¯äº‘å®Œæˆ: {cache_key}, {len(results)}ä¸ªè¯")

        except Exception as e:
            logger.error(f"é¢„è®¡ç®—è¯äº‘å¤±è´¥: {e}")

    @classmethod
    async def get_wordcloud_data(
        cls,
        hours: int = 24,
        top_n: int = 50,
        source: str = None
    ) -> List[Dict]:
        """è·å–è¯äº‘æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        # å…ˆå°è¯•ä»ç¼“å­˜è·å–
        cached = await cls.get_cached_wordcloud(hours, source)
        if cached:
            return cached[:top_n]

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œå®æ—¶è®¡ç®—
        logger.warning(f"âš ï¸ ç¼“å­˜æœªå‘½ä¸­ï¼Œå®æ—¶è®¡ç®—è¯äº‘: {hours}h")
        db = get_mongo_db()
        collection = db["market_news_enhanced"]

        query = {"dataTime": {"$gte": datetime.now() - timedelta(hours=hours)}}
        if source:
            query["source"] = source

        pipeline = [
            {"$match": query},
            {"$unwind": "$keywords"},
            {"$group": {"_id": "$keywords", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": top_n}
        ]

        results = []
        async for doc in collection.aggregate(pipeline):
            results.append({"word": doc["_id"], "weight": doc["count"], "count": doc["count"]})

        return results
```

**ä¿®æ”¹è¯äº‘æŸ¥è¯¢æ¥å£**:
```python
# app/routers/market_news.py
from app.services.wordcloud_cache_service import WordcloudCacheService

@router.get("/enhanced-wordcloud")
async def get_enhanced_wordcloud(
    hours: int = Query(24, description="æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰"),
    source: str = Query(None, description="æ•°æ®æ¥æº")
):
    """è·å–è¯äº‘æ•°æ®ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰"""
    results = await WordcloudCacheService.get_wordcloud_data(
        hours=hours,
        top_n=50,
        source=source
    )
    return {"data": results}
```

**æ·»åŠ å®šæ—¶ä»»åŠ¡**:
```python
# app/scheduler_service.py
@app.service.scheduled_job('interval', minutes=30)
async def refresh_wordcloud_cache():
    """æ¯30åˆ†é’Ÿåˆ·æ–°è¯äº‘ç¼“å­˜"""
    await WordcloudCacheService.precompute_wordcloud()
```

---

### ä¼˜åŒ– 4: åˆ†æ‰¹å¤„ç†å®æ—¶è¡Œæƒ…å…¥åº“

**æ–‡ä»¶**: `app/services/quotes_ingestion_service.py`

**ä¿®æ”¹ä½ç½®**: ç¬¬ 367-411 è¡Œ `_bulk_upsert` æ–¹æ³•

**ä¼˜åŒ–åä»£ç **:
```python
async def _bulk_upsert(
    self,
    quotes_map: Dict[str, Dict],
    trade_date: str,
    source: Optional[str] = None
) -> None:
    """æ‰¹é‡æ›´æ–°è¡Œæƒ…æ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†ä¼˜åŒ–ï¼‰"""
    db = get_mongo_db()
    coll = db[self.collection_name]

    BATCH_SIZE = 500  # æ¯æ‰¹500æ¡
    all_ops = []
    updated_at = datetime.now(self.tz)

    for code, q in quotes_map.items():
        if not code:
            continue

        code6 = self._normalize_stock_code(code)
        if not code6:
            continue

        # æ—¥å¿—è®°å½•
        if code6 in ["300750", "000001", "600000"]:
            logger.info(f"ğŸ“Š [å†™å…¥market_quotes] {code6} - volume={q.get('volume')}, source={source}")

        all_ops.append(
            UpdateOne(
                {"code": code6},
                {"$set": {
                    "code": code6,
                    "symbol": code6,
                    "close": q.get("close"),
                    "pct_chg": q.get("pct_chg"),
                    "amount": q.get("amount"),
                    "volume": q.get("volume"),
                    "open": q.get("open"),
                    "high": q.get("high"),
                    "low": q.get("low"),
                    "pre_close": q.get("pre_close"),
                    "trade_date": trade_date,
                    "updated_at": updated_at,
                }},
                upsert=True,
            )
        )

    if not all_ops:
        logger.info("æ— å¯å†™å…¥çš„æ•°æ®ï¼Œè·³è¿‡")
        return

    # åˆ†æ‰¹æ‰§è¡Œï¼Œé¿å…å†…å­˜æº¢å‡º
    total_matched = 0
    total_upserted = 0
    total_modified = 0

    for i in range(0, len(all_ops), BATCH_SIZE):
        batch = all_ops[i:i + BATCH_SIZE]
        try:
            result = await coll.bulk_write(batch, ordered=False)
            total_matched += result.matched_count
            total_upserted += len(result.upserted_ids) if result.upserted_ids else 0
            total_modified += result.modified_count

            logger.info(
                f"âœ… æ‰¹æ¬¡ {i//BATCH_SIZE + 1} å®Œæˆ: "
                f"matched={result.matched_count}, "
                f"upserted={len(result.upserted_ids) if result.upserted_ids else 0}, "
                f"modified={result.modified_count}"
            )
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {i//BATCH_SIZE + 1} å¤±è´¥: {e}")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
            continue

    logger.info(
        f"âœ… è¡Œæƒ…å…¥åº“å®Œæˆ source={source}, "
        f"total_matched={total_matched}, "
        f"total_upserted={total_upserted}, "
        f"total_modified={total_modified}"
    )
```

---

## P1 ä¼˜åŒ– - æœ¬æœˆå®Œæˆ

### ä¼˜åŒ– 5: å®ç°é…ç½®ç¼“å­˜å±‚

**æ–°å»ºæ–‡ä»¶**: `app/core/config_cache.py`

```python
"""
é…ç½®ç¼“å­˜æœåŠ¡
å‡å°‘é¢‘ç¹çš„æ•°æ®åº“é…ç½®è¯»å–
"""
import time
import logging
from typing import Dict, Any, Optional
from threading import Lock

logger = logging.getLogger(__name__)


class ConfigCache:
    """é…ç½®ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""

    def __init__(self, default_ttl: int = 300):  # é»˜è®¤5åˆ†é’Ÿ
        self._cache: Dict[str, Any] = {}
        self._timestamps: Dict[str, float] = {}
        self._ttl: int = default_ttl
        self._lock = Lock()

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        with self._lock:
            if key in self._cache:
                if time.time() - self._timestamps[key] < self._ttl:
                    logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {key}")
                    return self._cache[key]
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                    del self._cache[key]
                    del self._timestamps[key]
        return None

    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """è®¾ç½®ç¼“å­˜å€¼"""
        with self._lock:
            self._cache[key] = value
            self._timestamps[key] = time.time()
            if ttl:
                # ä¸ºå•ä¸ª key è®¾ç½®ä¸åŒçš„ TTL
                # æ³¨æ„ï¼šè¿™åªæ˜¯æ ‡è®°ï¼Œå®é™…æ£€æŸ¥æ—¶éœ€è¦é¢å¤–å¤„ç†
                pass
        logger.debug(f"ğŸ’¾ ç¼“å­˜å·²è®¾ç½®: {key}")

    def invalidate(self, key: Optional[str] = None) -> None:
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        with self._lock:
            if key:
                self._cache.pop(key, None)
                self._timestamps.pop(key, None)
                logger.debug(f"ğŸ—‘ï¸ ç¼“å­˜å·²å¤±æ•ˆ: {key}")
            else:
                self._cache.clear()
                self._timestamps.clear()
                logger.debug("ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤")

    def has(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
        with self._lock:
            if key in self._cache:
                if time.time() - self._timestamps[key] < self._ttl:
                    return True
            return False


# å…¨å±€å•ä¾‹
_config_cache = ConfigCache()


def get_config_cache() -> ConfigCache:
    """è·å–é…ç½®ç¼“å­˜å®ä¾‹"""
    return _config_cache
```

**ä¿®æ”¹é…ç½®æœåŠ¡ä½¿ç”¨ç¼“å­˜**:
```python
# app/services/config_service.py
from app.core.config_cache import get_config_cache

class ConfigService:
    async def get_system_config(self) -> SystemConfig:
        cache = get_config_cache()
        cache_key = "system_config"

        # å…ˆæ£€æŸ¥ç¼“å­˜
        cached = cache.get(cache_key)
        if cached:
            return cached

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“è¯»å–
        db = get_mongo_db()
        doc = await db.system_configs.find_one({"is_active": True})
        config = self._doc_to_config(doc)

        # å†™å…¥ç¼“å­˜
        cache.set(cache_key, config)

        return config

    async def update_system_config(self, config: SystemConfig) -> bool:
        # æ›´æ–°æ•°æ®åº“
        success = await self._save_to_db(config)

        if success:
            # ä½¿ç¼“å­˜å¤±æ•ˆ
            cache = get_config_cache()
            cache.invalidate("system_config")

        return success
```

---

### ä¼˜åŒ– 6: ä¿®å¤ LLM å®ä¾‹ç¼“å­˜å†…å­˜æ³„æ¼

**æ–‡ä»¶**: `app/services/analysis_service.py`

**ä¿®æ”¹ä½ç½®**: ç¬¬ 60 è¡Œ

**å½“å‰ä»£ç **:
```python
self._trading_graph_cache = {}
```

**ä¼˜åŒ–åä»£ç **:
```python
from cachetools import TTLCache

class AnalysisService:
    def __init__(self):
        # ... å…¶ä»–åˆå§‹åŒ– ...

        # ä½¿ç”¨ TTL ç¼“å­˜ï¼Œè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ¡ç›®
        self._trading_graph_cache = TTLCache(
            maxsize=50,      # æœ€å¤šç¼“å­˜ 50 ä¸ªä¸åŒé…ç½®
            ttl=3600         # 1å°æ—¶è¿‡æœŸ
        )
```

---

### ä¼˜åŒ– 7: æ·»åŠ æ•°æ®åº“ç´¢å¼•

**æ–°å»ºæ–‡ä»¶**: `app/services/database_index_service.py`

```python
"""
æ•°æ®åº“ç´¢å¼•ç®¡ç†æœåŠ¡
"""
import logging
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.database import get_mongo_db

logger = logging.getLogger(__name__)


class DatabaseIndexService:
    """æ•°æ®åº“ç´¢å¼•ç®¡ç†æœåŠ¡"""

    # ç´¢å¼•å®šä¹‰
    INDEXES = {
        "stock_basic_info": [
            [("source", 1), ("code", 1)],
            [("source", 1), ("symbol", 1)],
        ],
        "analysis_tasks": [
            [("user_id", 1), ("status", 1)],
            [("status", 1), ("created_at", -1)],
            [("batch_id", 1)],
            [("task_id", 1)],
        ],
        "market_news_enhanced": [
            [("hotnessScore", -1), ("category", 1)],
        ],
        "market_quotes": [
            [("code", 1), ("updated_at", -1)],
        ],
    }

    @classmethod
    async def ensure_indexes(cls):
        """ç¡®ä¿æ‰€æœ‰ç´¢å¼•å­˜åœ¨"""
        try:
            db = get_mongo_db()

            for collection_name, indexes in cls.INDEXES.items():
                collection = db[collection_name]

                for index_spec in indexes:
                    try:
                        await collection.create_index(index_spec)
                        logger.info(f"âœ… ç´¢å¼•å·²åˆ›å»º: {collection_name}.{index_spec}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ç´¢å¼•åˆ›å»ºå¤±è´¥: {collection_name}.{index_spec}, {e}")

            logger.info("âœ… æ•°æ®åº“ç´¢å¼•æ£€æŸ¥å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ç´¢å¼•æ£€æŸ¥å¤±è´¥: {e}")

    @classmethod
    async def analyze_slow_queries(cls, threshold_ms: int = 100):
        """åˆ†ææ…¢æŸ¥è¯¢ï¼ˆéœ€è¦å¯ç”¨ MongoDB Profilerï¼‰"""
        try:
            db = get_mongo_db()

            # æ£€æŸ¥ Profiler çŠ¶æ€
            profiler_status = await db.command("profile", -1)
            level = profiler_status.get("was", 0)

            if level == 0:
                logger.info("MongoDB Profiler æœªå¯ç”¨")
                logger.info("å¯ç”¨å‘½ä»¤: db.setProfilingLevel(1, {slowms: 100})")
                return

            # æŸ¥è¯¢æ…¢æŸ¥è¯¢
            slow_queries = await db.system.profile.find(
                {"millis": {"$gt": threshold_ms}}
            ).to_list(length=50)

            if slow_queries:
                logger.warning(f"âš ï¸ å‘ç° {len(slow_queries)} ä¸ªæ…¢æŸ¥è¯¢:")
                for sq in slow_queries[:10]:
                    logger.warning(
                        f"  - {sq.get('ns')}: {sq.get('millis')}ms - "
                        f"{sq.get('command', {}).get('filter')}"
                    )

        except Exception as e:
            logger.error(f"åˆ†ææ…¢æŸ¥è¯¢å¤±è´¥: {e}")
```

---

### ä¼˜åŒ– 8: ä¼˜åŒ– Redis è¿æ¥æ± 

**æ–‡ä»¶**: `app/core/redis_client.py`

**ä¿®æ”¹è¿æ¥æ± é…ç½®**:
```python
from redis.asyncio import ConnectionPool, Redis
from app.core.config import settings

# å…¨å±€è¿æ¥æ± 
_redis_pool: Optional[ConnectionPool] = None
_redis_client: Optional[Redis] = None


def get_redis_client() -> Redis:
    """è·å– Redis å®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼Œå¸¦è¿æ¥æ± ï¼‰"""
    global _redis_pool, _redis_client

    if _redis_client is None:
        _redis_pool = ConnectionPool(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB,
            password=settings.REDIS_PASSWORD if hasattr(settings, 'REDIS_PASSWORD') else None,
            max_connections=50,          # å¢åŠ æœ€å¤§è¿æ¥æ•°
            socket_keepalive=True,       # ä¿æŒè¿æ¥æ´»è·ƒ
            socket_connect_timeout=5,    # è¿æ¥è¶…æ—¶ 5 ç§’
            socket_timeout=5,            # è¯»å†™è¶…æ—¶ 5 ç§’
            retry_on_timeout=True,       # è¶…æ—¶è‡ªåŠ¨é‡è¯•
            health_check_interval=30,    # æ¯30ç§’å¥åº·æ£€æŸ¥
            decode_responses=True,       # è‡ªåŠ¨è§£ç ä¸ºå­—ç¬¦ä¸²
        )

        _redis_client = Redis(connection_pool=_redis_pool)

    return _redis_client


async def close_redis_client():
    """å…³é—­ Redis è¿æ¥ï¼ˆåº”ç”¨å…³é—­æ—¶è°ƒç”¨ï¼‰"""
    global _redis_pool, _redis_client

    if _redis_client:
        await _redis_client.close()
        _redis_client = None

    if _redis_pool:
        await _redis_pool.aclose()  # å¼‚æ­¥å…³é—­è¿æ¥æ± 
        _redis_pool = None
```

---

### ä¼˜åŒ– 9: å‰ç«¯è¯·æ±‚åˆå¹¶

**æ–°å»ºæ–‡ä»¶**: `frontend/src/utils/apiCache.ts`

```typescript
/**
 * API è¯·æ±‚ç¼“å­˜å·¥å…·
 * åˆå¹¶é‡å¤è¯·æ±‚ï¼Œå‡å°‘æœåŠ¡å™¨è´Ÿè½½
 */

interface CachedRequest {
  promise: Promise<any>;
  timestamp: number;
}

export class ApiCache {
  private cache = new Map<string, CachedRequest>();
  private ttl: number; // æ¯«ç§’

  constructor(ttl: number = 5000) {
    this.ttl = ttl;
    // å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
    setInterval(() => this.cleanup(), this.ttl);
  }

  async fetch<T>(key: string, fetcher: () => Promise<T>): Promise<T> {
    const cached = this.cache.get(key);

    if (cached) {
      const age = Date.now() - cached.timestamp;
      if (age < this.ttl) {
        console.log(`[API Cache] å‘½ä¸­: ${key}`);
        return cached.promise;
      }
    }

    // åˆ›å»ºæ–°è¯·æ±‚
    console.log(`[API Cache] æœªå‘½ä¸­: ${key}`);
    const promise = fetcher();
    this.cache.set(key, {
      promise,
      timestamp: Date.now()
    });

    return promise;
  }

  invalidate(key?: string): void {
    if (key) {
      this.cache.delete(key);
      console.log(`[API Cache] å¤±æ•ˆ: ${key}`);
    } else {
      this.cache.clear();
      console.log(`[API Cache] å…¨éƒ¨æ¸…é™¤`);
    }
  }

  private cleanup(): void {
    const now = Date.now();
    const keysToDelete: string[] = [];

    this.cache.forEach((value, key) => {
      if (now - value.timestamp >= this.ttl) {
        keysToDelete.push(key);
      }
    });

    keysToDelete.forEach(key => this.cache.delete(key));

    if (keysToDelete.length > 0) {
      console.log(`[API Cache] æ¸…ç† ${keysToDelete.length} ä¸ªè¿‡æœŸç¼“å­˜`);
    }
  }
}

// å…¨å±€å•ä¾‹
export const apiCache = new ApiCache(5000); // 5ç§’ç¼“å­˜
```

---

### ä¼˜åŒ– 10: å®ç°æ¸¸æ ‡åˆ†é¡µ

**æ–°å»ºæ–‡ä»¶**: `app/services/pagination_service.py`

```python
"""
æ¸¸æ ‡åˆ†é¡µæœåŠ¡
ä¼˜åŒ–å¤§åç§»é‡åˆ†é¡µæ€§èƒ½
"""
from typing import List, Optional, Dict, Any
from bson import ObjectId
import logging

logger = logging.getLogger(__name__)


class CursorPagination:
    """æ¸¸æ ‡åˆ†é¡µ"""

    @staticmethod
    async def paginate(
        collection,
        query: Dict[str, Any],
        sort: List[tuple],
        page_size: int = 20,
        cursor: Optional[str] = None,
        previous: bool = False
    ) -> Dict[str, Any]:
        """
        æ¸¸æ ‡åˆ†é¡µæŸ¥è¯¢

        Args:
            collection: MongoDB é›†åˆ
            query: æŸ¥è¯¢æ¡ä»¶
            sort: æ’åºå­—æ®µï¼Œå¦‚ [("created_at", -1)]
            page_size: æ¯é¡µæ•°é‡
            cursor: æ¸¸æ ‡ï¼ˆä¸Šä¸€é¡µè¿”å›çš„ next_cursorï¼‰
            previous: æ˜¯å¦æŸ¥è¯¢ä¸Šä¸€é¡µ

        Returns:
            {
                "items": [...],
                "next_cursor": "...",
                "has_next": True/False,
                "has_prev": True/False
            }
        """
        # è§£ææ¸¸æ ‡
        cursor_filter = {}
        if cursor:
            try:
                cursor_obj = ObjectId(cursor)
                if previous:
                    # æŸ¥è¯¢ä¸Šä¸€é¡µ
                    cursor_filter["_id"] = {"$lt": cursor_obj}
                else:
                    # æŸ¥è¯¢ä¸‹ä¸€é¡µ
                    cursor_filter["_id"] = {"$gt": cursor_obj}
            except Exception:
                logger.warning(f"æ— æ•ˆçš„æ¸¸æ ‡: {cursor}")

        # åˆå¹¶æŸ¥è¯¢æ¡ä»¶
        final_query = {**query, **cursor_filter}

        # æ‰§è¡ŒæŸ¥è¯¢
        cursor_obj = collection.find(final_query).sort(sort).limit(page_size + 1)

        items = await cursor_obj.to_list(length=page_size + 1)

        # åˆ¤æ–­æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        has_next = len(items) > page_size
        has_prev = cursor is not None

        # ç§»é™¤å¤šå‡ºçš„ä¸€é¡¹
        if has_next:
            items = items[:page_size]

        # ç”Ÿæˆä¸‹ä¸€é¡µæ¸¸æ ‡
        next_cursor = None
        if items:
            next_cursor = str(items[-1]["_id"])

        return {
            "items": items,
            "next_cursor": next_cursor,
            "has_next": has_next,
            "has_prev": has_prev,
            "page_size": page_size
        }
```

---

## P2 ä¼˜åŒ– - å­£åº¦å†…å®Œæˆ

### ä¼˜åŒ– 11: ä¼˜åŒ–æ—¥å¿—è¾“å‡º

**ä¿®æ”¹**: `app/core/logging_config.py`

```python
import logging
import os

# æ ¹æ®ç¯å¢ƒè®¾ç½®æ—¥å¿—çº§åˆ«
ENV = os.getenv("ENV", "development")
DEFAULT_LOG_LEVEL = "WARNING" if ENV == "production" else "DEBUG"

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=DEFAULT_LOG_LEVEL,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
```

---

### ä¼˜åŒ– 12: æ€§èƒ½ç›‘æ§é›†æˆ

**æ–°å»ºæ–‡ä»¶**: `app/middleware/performance_monitor.py`

```python
"""
æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
"""
import time
import logging
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware

logger = logging.getLogger(__name__)


class PerformanceMonitorMiddleware(BaseHTTPMiddleware):
    """æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶"""

    async def dispatch(self, request: Request, call_next):
        start_time = time.time()

        # å¤„ç†è¯·æ±‚
        response = await call_next(request)

        # è®¡ç®—è€—æ—¶
        process_time = time.time() - start_time
        response.headers["X-Process-Time"] = str(process_time)

        # è®°å½•æ…¢è¯·æ±‚
        if process_time > 1.0:  # è¶…è¿‡1ç§’
            logger.warning(
                f"âš ï¸ æ…¢è¯·æ±‚: {request.method} {request.url.path} "
                f"è€—æ—¶ {process_time:.2f}ç§’"
            )

        return response
```

---

## å®æ–½è®¡åˆ’

### Week 1: P0 ä¼˜åŒ–

| æ—¥æœŸ | ä»»åŠ¡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| Day 1 | ä¼˜åŒ–1: N+1æŸ¥è¯¢ä¿®å¤ | - | å¾…å¼€å§‹ |
| Day 1 | ä¼˜åŒ–2: æ‰¹é‡ä»»åŠ¡å…¥é˜Ÿ | - | å¾…å¼€å§‹ |
| Day 2 | ä¼˜åŒ–3: è¯äº‘é¢„èšåˆ | - | å¾…å¼€å§‹ |
| Day 2 | ä¼˜åŒ–4: è¡Œæƒ…å…¥åº“åˆ†æ‰¹ | - | å¾…å¼€å§‹ |
| Day 3 | æµ‹è¯•éªŒè¯ | - | å¾…å¼€å§‹ |

### Week 2: P1 ä¼˜åŒ–

| æ—¥æœŸ | ä»»åŠ¡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| Day 1 | ä¼˜åŒ–5: é…ç½®ç¼“å­˜ | - | å¾…å¼€å§‹ |
| Day 1 | ä¼˜åŒ–6: LLMç¼“å­˜ä¿®å¤ | - | å¾…å¼€å§‹ |
| Day 2 | ä¼˜åŒ–7: æ•°æ®åº“ç´¢å¼• | - | å¾…å¼€å§‹ |
| Day 2 | ä¼˜åŒ–8: Redisè¿æ¥æ±  | - | å¾…å¼€å§‹ |
| Day 3 | ä¼˜åŒ–9: å‰ç«¯è¯·æ±‚åˆå¹¶ | - | å¾…å¼€å§‹ |
| Day 3 | ä¼˜åŒ–10: æ¸¸æ ‡åˆ†é¡µ | - | å¾…å¼€å§‹ |

### Week 3: P2 ä¼˜åŒ–

| æ—¥æœŸ | ä»»åŠ¡ | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| Day 1 | ä¼˜åŒ–11: æ—¥å¿—ä¼˜åŒ– | - | å¾…å¼€å§‹ |
| Day 1 | ä¼˜åŒ–12: æ€§èƒ½ç›‘æ§ | - | å¾…å¼€å§‹ |
| Day 2 | å…¨é¢æµ‹è¯• | - | å¾…å¼€å§‹ |
| Day 3 | æ€§èƒ½åŸºå‡†å¯¹æ¯” | - | å¾…å¼€å§‹ |

---

## æµ‹è¯•æ–¹æ¡ˆ

### 1. å•å…ƒæµ‹è¯•

```python
# tests/test_performance_optimizations.py

import pytest
import asyncio
from app.services.stock_data_service import StockDataService
from app.services.queue_service import QueueService

@pytest.mark.asyncio
async def test_stock_query_n1_fix():
    """æµ‹è¯•N+1æŸ¥è¯¢ä¿®å¤æ•ˆæœ"""
    service = StockDataService()

    # æµ‹è¯•100æ¬¡æŸ¥è¯¢
    import time
    symbols = [f"{str(i).zfill(6)}" for i in range(1, 101)]

    start = time.time()
    for symbol in symbols:
        await service.get_stock_basic_info(symbol)
    elapsed = time.time() - start

    # é¢„æœŸï¼š100æ¬¡æŸ¥è¯¢åº”åœ¨ 5 ç§’å†…å®Œæˆ
    assert elapsed < 5, f"100æ¬¡æŸ¥è¯¢è€—æ—¶ {elapsed:.2f}ç§’ï¼Œè¶…è¿‡é¢„æœŸ"
    print(f"âœ… 100æ¬¡æŸ¥è¯¢è€—æ—¶: {elapsed:.2f}ç§’")

@pytest.mark.asyncio
async def test_batch_enqueue_performance():
    """æµ‹è¯•æ‰¹é‡å…¥é˜Ÿæ€§èƒ½"""
    queue = QueueService(get_redis_client())

    import time
    symbols = [f"{str(i).zfill(6)}" for i in range(1, 101)]

    start = time.time()
    batch_id, count = await queue.create_batch(
        user_id="test_user",
        symbols=symbols,
        params={}
    )
    elapsed = time.time() - start

    # é¢„æœŸï¼š100ä»»åŠ¡å…¥é˜Ÿåº”åœ¨ 50ms å†…å®Œæˆ
    assert elapsed < 0.05, f"100ä»»åŠ¡å…¥é˜Ÿè€—æ—¶ {elapsed*1000:.2f}msï¼Œè¶…è¿‡é¢„æœŸ"
    print(f"âœ… 100ä»»åŠ¡å…¥é˜Ÿè€—æ—¶: {elapsed*1000:.2f}ms")
```

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# tests/benchmark.py

import asyncio
import time
from app.services.news_database_service import NewsDatabaseService
from app.services.wordcloud_cache_service import WordcloudCacheService

async def benchmark_wordcloud():
    """è¯äº‘æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    # æµ‹è¯•æœªä¼˜åŒ–ç‰ˆæœ¬
    print("æµ‹è¯•æœªä¼˜åŒ–ç‰ˆæœ¬...")
    start = time.time()
    for _ in range(10):
        await NewsDatabaseService.get_wordcloud_data(hours=24, top_n=50)
    elapsed_old = time.time() - start
    print(f"æœªä¼˜åŒ–: 10æ¬¡æŸ¥è¯¢è€—æ—¶ {elapsed_old:.2f}ç§’")

    # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
    print("\næµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬...")
    start = time.time()
    for _ in range(10):
        await WordcloudCacheService.get_wordcloud_data(hours=24, top_n=50)
    elapsed_new = time.time() - start
    print(f"ä¼˜åŒ–å: 10æ¬¡æŸ¥è¯¢è€—æ—¶ {elapsed_new:.2f}ç§’")

    # æ€§èƒ½æå‡
    improvement = (elapsed_old - elapsed_new) / elapsed_old * 100
    print(f"\nâœ… æ€§èƒ½æå‡: {improvement:.1f}%")

if __name__ == "__main__":
    asyncio.run(benchmark_wordcloud())
```

---

## å›æ»šè®¡åˆ’

å¦‚æœä¼˜åŒ–åå‡ºç°é—®é¢˜ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤å›æ»šï¼š

1. **ç«‹å³å›æ»š**: Git revert åˆ°ä¼˜åŒ–å‰çš„ commit
2. **é—®é¢˜åˆ†æ**: æŸ¥çœ‹æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯
3. **ä¿®å¤åå†å°è¯•**: ä¿®å¤é—®é¢˜åå†æ¬¡åº”ç”¨ä¼˜åŒ–

---

## æ€»ç»“

æœ¬ä¼˜åŒ–æ–¹æ¡ˆæ¶µç›–äº† 12 é¡¹æ€§èƒ½ä¼˜åŒ–ï¼Œé¢„è®¡æ”¶ç›Šï¼š

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| è‚¡ç¥¨æŸ¥è¯¢å“åº” | 200-500ms | 50-100ms | 75% â¬‡ï¸ |
| æ‰¹é‡å…¥é˜Ÿåå | 100ä»»åŠ¡/100ms | 100ä»»åŠ¡/10ms | 10å€ â¬†ï¸ |
| è¯äº‘æŸ¥è¯¢æ—¶é—´ | 2-5ç§’ | 50-100ms | 95% â¬‡ï¸ |
| é…ç½®è¯»å–æ—¶é—´ | 50-100ms | 1-2ms | 98% â¬‡ï¸ |
| æ•°æ®åº“è´Ÿè½½ | 100% | 30-50% | 50-70% â¬‡ï¸ |
